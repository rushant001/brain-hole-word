"""
Brain-Hole-Word Agent Nodes
ä½¿ç”¨ LangChain 1.2.0 çš„æ–°ç‰¹æ€§ï¼š
- with_structured_output() è¿›è¡Œç»“æ„åŒ–è¾“å‡º
- æ”¯æŒå¤šç§ç”Ÿå›¾æ¨¡å‹ (DALL-E, FLUX, Qwen, Muse)
- ç²¾ç¡®çš„éŸ³è§†é¢‘åŒæ­¥æœºåˆ¶
"""
import os
import asyncio
import httpx
from pathlib import Path
from typing import Literal, List

from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage
from openai import OpenAI
from pydub import AudioSegment

from .state import AgentState, MnemonicContent, VisualPlan, AudioTiming
from .utils import CONFIG, get_output_path
from .prompts import (
    CREATIVE_SYSTEM_PROMPT, CREATIVE_USER_PROMPT,
    VISUAL_SYSTEM_PROMPT, VISUAL_USER_PROMPT
)

# === LLM Initialization with Structured Output ===

def get_creative_llm():
    """è·å–åˆ›æ„ç”Ÿæˆ LLMï¼Œä½¿ç”¨ç»“æ„åŒ–è¾“å‡º"""
    base_url = CONFIG['llm'].get('base_url')
    llm = ChatOpenAI(
        model=CONFIG['llm']['creative_model'],
        temperature=CONFIG['llm']['temperature'],
        base_url=base_url
    )
    return llm.with_structured_output(MnemonicContent)

def get_visual_llm():
    """è·å–è§†è§‰è§„åˆ’ LLMï¼Œä½¿ç”¨ç»“æ„åŒ–è¾“å‡º"""
    base_url = CONFIG['llm'].get('base_url')
    llm = ChatOpenAI(
        model=CONFIG['llm']['visual_model'],
        temperature=0.7,
        base_url=base_url
    )
    return llm.with_structured_output(VisualPlan)


# === Node 1: Creative_Brain ===

def creative_brain_node(state: AgentState) -> dict:
    """åˆ›æ„ç­–åˆ’èŠ‚ç‚¹"""
    word = state['word']
    phonetic = state.get('phonetic', '')
    
    print(f"ğŸ§  [Creative_Brain] æ­£åœ¨ä¸º '{word}' æ„æ€è„‘æ´è®°å¿†æ³•...")
    
    llm = get_creative_llm()
    
    messages = [
        SystemMessage(content=CREATIVE_SYSTEM_PROMPT),
        HumanMessage(content=CREATIVE_USER_PROMPT.format(word=word, phonetic=phonetic))
    ]
    
    result: MnemonicContent = llm.invoke(messages)
    
    print(f"   âœ… ç­–ç•¥: {result.strategy}")
    print(f"   âœ… å£å·: {result.slogan}")
    print(f"   âœ… è„šæœ¬åˆ†æ®µ: {len(result.narration_segments)} æ®µ")
    
    return {
        "mnemonic": result,
        "current_step": "creative_brain_done"
    }


# === Node 2: Visual_Planner ===

def visual_planner_node(state: AgentState) -> dict:
    """è§†è§‰è§„åˆ’èŠ‚ç‚¹"""
    mnemonic = state.get('mnemonic')
    if not mnemonic:
        return {"error": "No mnemonic content found"}
    
    print(f"ğŸ¨ [Visual_Planner] æ­£åœ¨ç”Ÿæˆç»˜å›¾ Prompt...")
    
    llm = get_visual_llm()
    
    messages = [
        SystemMessage(content=VISUAL_SYSTEM_PROMPT),
        HumanMessage(content=VISUAL_USER_PROMPT.format(
            word=state['word'],
            slogan=mnemonic.slogan,
            story_scene=mnemonic.story_scene
        ))
    ]
    
    result: VisualPlan = llm.invoke(messages)
    
    print(f"   âœ… ä¸»åœºæ™¯ Prompt: {result.main_scene_prompt[:80]}...")
    
    return {
        "visual_plan": result,
        "current_step": "visual_planner_done"
    }


# === Node 3: Image_Generator (å¤šæ¨¡å‹æ”¯æŒ) ===

def image_generator_node(state: AgentState) -> dict:
    """å›¾ç‰‡ç”ŸæˆèŠ‚ç‚¹ï¼šæ”¯æŒå¤šç§ç”Ÿå›¾æ¨¡å‹"""
    
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨æ‰‹åŠ¨å›¾ç‰‡
    if state.get('use_manual_image') and state.get('manual_image_url'):
        print(f"ğŸ–¼ï¸ [Image_Generator] ä½¿ç”¨ç”¨æˆ·æä¾›çš„å›¾ç‰‡...")
        image_path = download_image(state['manual_image_url'], state['word'])
        return {"main_image_path": str(image_path)}
    
    visual_plan = state.get('visual_plan')
    if not visual_plan:
        return {"error": "No visual plan found"}
    
    provider = CONFIG['image_generation']['provider']
    print(f"ğŸ–¼ï¸ [Image_Generator] ä½¿ç”¨ {provider} ç”Ÿæˆå›¾ç‰‡...")
    
    prompt = visual_plan.main_scene_prompt
    
    if provider == 'dall-e-3':
        image_url = generate_dalle(prompt)
    elif provider == 'flux':
        image_url = generate_flux(prompt)
    elif provider == 'qwen':
        image_url = generate_qwen(prompt)
    elif provider == 'muse':
        image_url = generate_muse(prompt)
    else:
        return {"error": f"Unknown provider: {provider}"}
    
    if image_url:
        image_path = download_image(image_url, state['word'])
        print(f"   âœ… å›¾ç‰‡å·²ä¿å­˜: {image_path}")
        return {"main_image_path": str(image_path)}
    else:
        return {"error": "Image generation failed"}


def generate_dalle(prompt: str) -> str:
    """DALL-E 3 ç”Ÿå›¾"""
    config = CONFIG['image_generation']['dalle']
    client = OpenAI()
    response = client.images.generate(
        model="dall-e-3",
        prompt=prompt,
        size=config['size'],
        quality=config['quality'],
        style=config['style'],
        n=1
    )
    return response.data[0].url


def generate_flux(prompt: str) -> str:
    """FLUX.1-schnell ç”Ÿå›¾ (å…¼å®¹ OpenAI API æ ¼å¼)"""
    config = CONFIG['image_generation']['flux']
    client = OpenAI(
        base_url=config['base_url'],
        api_key=os.getenv("FLUX_API_KEY", os.getenv("OPENAI_API_KEY"))
    )
    response = client.images.generate(
        model=config['model'],
        prompt=prompt,
        size=config['size'],
        n=1
    )
    return response.data[0].url


def generate_qwen(prompt: str) -> str:
    """Qwen/é€šä¹‰ä¸‡è±¡ ç”Ÿå›¾"""
    config = CONFIG['image_generation']['qwen']
    client = OpenAI(
        base_url=config['base_url'],
        api_key=os.getenv("DASHSCOPE_API_KEY", os.getenv("OPENAI_API_KEY"))
    )
    response = client.images.generate(
        model=config['model'],
        prompt=prompt,
        size=config['size'],
        n=1
    )
    return response.data[0].url


def generate_muse(prompt: str) -> str:
    """MuseSteamer-Air-Image ç”Ÿå›¾"""
    config = CONFIG['image_generation']['muse']
    client = OpenAI(
        base_url=config['base_url'],
        api_key=os.getenv("MUSE_API_KEY", os.getenv("OPENAI_API_KEY"))
    )
    response = client.images.generate(
        model=config['model'],
        prompt=prompt,
        size=config['size'],
        n=1
    )
    return response.data[0].url


def download_image(url: str, word: str) -> Path:
    """ä¸‹è½½å›¾ç‰‡åˆ°æœ¬åœ°"""
    output_path = get_output_path("images", f"{word}_main.png")
    response = httpx.get(url, follow_redirects=True, timeout=60)
    response.raise_for_status()
    with open(output_path, 'wb') as f:
        f.write(response.content)
    return output_path


# === Node 4: Card_Generator ===

def card_generator_node(state: AgentState) -> dict:
    """æ–‡å­—å¡ç‰‡ç”ŸæˆèŠ‚ç‚¹"""
    from PIL import Image, ImageDraw, ImageFont
    
    print(f"ğŸ“ [Card_Generator] ç”Ÿæˆæ–‡å­—å¡ç‰‡...")
    
    word = state['word']
    phonetic = state.get('phonetic', '')
    mnemonic = state.get('mnemonic')
    
    config = CONFIG['cards']
    width = CONFIG['video']['width']
    height = CONFIG['video']['height']
    
    # åŠ è½½å­—ä½“
    try:
        if config['font_path']:
            title_font = ImageFont.truetype(config['font_path'], config['title_font_size'])
            content_font = ImageFont.truetype(config['font_path'], config['content_font_size'])
        else:
            title_font = ImageFont.truetype("/System/Library/Fonts/PingFang.ttc", config['title_font_size'])
            content_font = ImageFont.truetype("/System/Library/Fonts/PingFang.ttc", config['content_font_size'])
    except:
        title_font = ImageFont.load_default()
        content_font = ImageFont.load_default()
    
    bg_color = config['background_color']
    text_color = config['text_color']
    accent_color = config['accent_color']
    
    # æ ‡é¢˜å¡
    title_card = Image.new('RGB', (width, height), bg_color)
    draw = ImageDraw.Draw(title_card)
    word_upper = word.upper()
    bbox = draw.textbbox((0, 0), word_upper, font=title_font)
    word_width = bbox[2] - bbox[0]
    word_x = (width - word_width) // 2
    draw.text((word_x, height // 3), word_upper, fill=accent_color, font=title_font)
    if phonetic:
        bbox = draw.textbbox((0, 0), phonetic, font=content_font)
        phonetic_width = bbox[2] - bbox[0]
        draw.text(((width - phonetic_width) // 2, height // 3 + 150), phonetic, fill=text_color, font=content_font)
    title_path = get_output_path("cards", f"{word}_title.png")
    title_card.save(title_path)
    
    # ä¾‹å¥å¡
    sentence_card = Image.new('RGB', (width, height), bg_color)
    draw = ImageDraw.Draw(sentence_card)
    if mnemonic:
        draw.text((80, height // 3), mnemonic.example_sentence_en, fill=accent_color, font=content_font)
        draw.text((80, height // 3 + 100), mnemonic.example_sentence_cn, fill=text_color, font=content_font)
    sentence_path = get_output_path("cards", f"{word}_sentence.png")
    sentence_card.save(sentence_path)
    
    # ç»“å°¾å¡
    ending_card = Image.new('RGB', (width, height), bg_color)
    draw = ImageDraw.Draw(ending_card)
    ending_text = "æ¯å¤©ä¸€ä¸ªè„‘æ´è¯\nå…³æ³¨æˆ‘"
    bbox = draw.textbbox((0, 0), ending_text, font=content_font)
    text_width = bbox[2] - bbox[0]
    draw.text(((width - text_width) // 2, height // 2 - 50), ending_text, fill=accent_color, font=content_font, align="center")
    ending_path = get_output_path("cards", f"{word}_ending.png")
    ending_card.save(ending_path)
    
    print(f"   âœ… å·²ç”Ÿæˆ 3 å¼ å¡ç‰‡")
    
    return {
        "title_card_path": str(title_path),
        "sentence_card_path": str(sentence_path),
        "ending_card_path": str(ending_path),
        "current_step": "card_generator_done"
    }


# === Node 5: Audio_Producer (å¸¦ç²¾ç¡®æ—¶é•¿) ===

def audio_producer_node(state: AgentState) -> dict:
    """
    éŸ³é¢‘åˆæˆèŠ‚ç‚¹ï¼šåˆ†æ®µç”ŸæˆéŸ³é¢‘ï¼Œå¹¶è®°å½•ç²¾ç¡®æ—¶é•¿ç”¨äºè§†é¢‘åŒæ­¥
    """
    print(f"ğŸ™ï¸ [Audio_Producer] ç”ŸæˆéŸ³é¢‘...")
    
    word = state['word']
    mnemonic = state.get('mnemonic')
    
    if not mnemonic:
        return {"error": "No mnemonic content found"}
    
    provider = CONFIG['audio']['provider']
    audio_timings = []
    current_start = 0.0
    
    if provider == 'edge-tts':
        for segment in mnemonic.narration_segments:
            segment_id = segment.segment_id
            text = segment.text
            
            # ç¡®å®šä½¿ç”¨çš„éŸ³è‰²
            if segment_id == "opening":
                voice = CONFIG['audio']['voice_en']
            else:
                voice = CONFIG['audio']['voice_cn']
            
            audio_path = get_output_path("audio", f"{word}_{segment_id}.mp3")
            
            # ç”ŸæˆéŸ³é¢‘
            asyncio.run(_generate_edge_tts_single(text, voice, str(audio_path)))
            
            # è·å–å®é™…æ—¶é•¿
            audio = AudioSegment.from_mp3(str(audio_path))
            duration = len(audio) / 1000.0  # è½¬æ¢ä¸ºç§’
            
            timing = {
                "segment_id": segment_id,
                "audio_path": str(audio_path),
                "duration_seconds": duration,
                "start_time": current_start
            }
            audio_timings.append(timing)
            
            print(f"   âœ… {segment_id}: {duration:.2f}s (ä» {current_start:.2f}s å¼€å§‹)")
            
            # ç´¯åŠ æ—¶é—´ (åŠ ä¸Šç‰‡æ®µé—´éš”)
            current_start += duration + CONFIG['video'].get('padding_seconds', 0.3)
        
        total_duration = current_start
        print(f"   ğŸ“Š æ€»æ—¶é•¿: {total_duration:.2f}s")
        
        return {
            "audio_timings": audio_timings,
            "total_audio_duration": total_duration,
            "current_step": "audio_producer_done"
        }
    else:
        return {"error": f"Unknown audio provider: {provider}"}


async def _generate_edge_tts_single(text: str, voice: str, output_path: str):
    """ç”Ÿæˆå•ä¸ª Edge TTS éŸ³é¢‘"""
    import edge_tts
    rate = CONFIG['audio']['rate']
    communicate = edge_tts.Communicate(text, voice, rate=rate)
    await communicate.save(output_path)


# === Node 6: Video_Composer (åŸºäºéŸ³é¢‘æ—¶é•¿åŠ¨æ€åˆæˆ) ===

def video_composer_node(state: AgentState) -> dict:
    """
    è§†é¢‘åˆæˆèŠ‚ç‚¹ï¼šåŸºäºéŸ³é¢‘å®é™…æ—¶é•¿åŠ¨æ€ç”Ÿæˆè§†é¢‘
    ç¡®ä¿éŸ³ç”»å®Œç¾åŒæ­¥
    """
    from moviepy import (
        ImageClip, AudioFileClip, CompositeVideoClip, 
        concatenate_videoclips, CompositeAudioClip
    )
    import numpy as np
    
    print(f"ğŸ¬ [Video_Composer] åˆæˆè§†é¢‘ (åŸºäºéŸ³é¢‘æ—¶é•¿)...")
    
    word = state['word']
    config = CONFIG['video']
    width, height = config['width'], config['height']
    fps = config['fps']
    zoom_factor = config['ken_burns_zoom']
    
    audio_timings = state.get('audio_timings', [])
    if not audio_timings:
        return {"error": "No audio timings found"}
    
    # æ„å»ºæ—¶é•¿æ˜ å°„
    timing_map = {t['segment_id']: t for t in audio_timings}
    
    main_image = state.get('main_image_path')
    title_card = state.get('title_card_path')
    sentence_card = state.get('sentence_card_path')
    ending_card = state.get('ending_card_path')
    
    if not all([main_image, title_card]):
        return {"error": "Missing required assets"}
    
    clips = []
    audio_clips = []
    
    # === Clip 1: Opening (æ ‡é¢˜å¡ + å•è¯å‘éŸ³) ===
    opening_timing = timing_map.get('opening', {'duration_seconds': 3, 'start_time': 0})
    opening_duration = opening_timing['duration_seconds'] + 0.5  # åŠ ç¼“å†²
    
    title_clip = ImageClip(title_card).set_duration(opening_duration).resize((width, height))
    clips.append(title_clip)
    
    if opening_timing.get('audio_path'):
        audio_clips.append(
            AudioFileClip(opening_timing['audio_path']).set_start(opening_timing['start_time'])
        )
    
    # === Clip 2: Mnemonic (ä¸»å›¾ + è„‘æ´è§£è¯´ + Ken Burns) ===
    mnemonic_timing = timing_map.get('mnemonic', {'duration_seconds': 15, 'start_time': 3})
    mnemonic_duration = mnemonic_timing['duration_seconds'] + 1  # åŠ ç¼“å†²
    
    def ken_burns_effect(get_frame, t):
        """Ken Burns æ¨æ‹‰æ•ˆæœ"""
        progress = t / mnemonic_duration
        current_zoom = 1 + (zoom_factor - 1) * progress
        
        frame = get_frame(t)
        h, w = frame.shape[:2]
        
        new_h = int(h / current_zoom)
        new_w = int(w / current_zoom)
        start_y = (h - new_h) // 2
        start_x = (w - new_w) // 2
        
        cropped = frame[start_y:start_y+new_h, start_x:start_x+new_w]
        
        from PIL import Image
        img = Image.fromarray(cropped)
        img = img.resize((w, h), Image.LANCZOS)
        return np.array(img)
    
    main_clip = ImageClip(main_image).set_duration(mnemonic_duration).resize((width, height))
    main_clip = main_clip.fl(ken_burns_effect, apply_to=['mask'])
    clips.append(main_clip)
    
    if mnemonic_timing.get('audio_path'):
        audio_clips.append(
            AudioFileClip(mnemonic_timing['audio_path']).set_start(mnemonic_timing['start_time'])
        )
    
    # === Clip 3: Sentence (ä¾‹å¥å¡) ===
    sentence_timing = timing_map.get('sentence', {'duration_seconds': 8, 'start_time': 18})
    sentence_duration = sentence_timing['duration_seconds'] + 0.5
    
    if sentence_card:
        sentence_clip = ImageClip(sentence_card).set_duration(sentence_duration).resize((width, height))
        clips.append(sentence_clip)
    
    if sentence_timing.get('audio_path'):
        audio_clips.append(
            AudioFileClip(sentence_timing['audio_path']).set_start(sentence_timing['start_time'])
        )
    
    # === Clip 4: Ending (ç»“å°¾å¡) ===
    ending_timing = timing_map.get('ending', {'duration_seconds': 4, 'start_time': 26})
    ending_duration = ending_timing['duration_seconds'] + 0.5
    
    if ending_card:
        ending_clip = ImageClip(ending_card).set_duration(ending_duration).resize((width, height))
        clips.append(ending_clip)
    
    if ending_timing.get('audio_path'):
        audio_clips.append(
            AudioFileClip(ending_timing['audio_path']).set_start(ending_timing['start_time'])
        )
    
    # æ‹¼æ¥è§†é¢‘
    final_video = concatenate_videoclips(clips, method="compose")
    
    # åˆæˆéŸ³é¢‘è½¨é“
    if audio_clips:
        combined_audio = CompositeAudioClip(audio_clips)
        final_video = final_video.set_audio(combined_audio)
    
    # è¾“å‡º
    output_path = get_output_path("video", f"{word}_final.mp4")
    final_video.write_videofile(
        str(output_path),
        fps=fps,
        codec='libx264',
        audio_codec='aac',
        verbose=False,
        logger=None
    )
    
    total_duration = state.get('total_audio_duration', sum(c.duration for c in clips))
    print(f"   âœ… è§†é¢‘å·²ç”Ÿæˆ: {output_path}")
    print(f"   ğŸ“Š è§†é¢‘æ€»æ—¶é•¿: {total_duration:.2f}s")
    
    return {
        "final_video_path": str(output_path),
        "current_step": "video_composer_done"
    }
